{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1                             2        3         4  \\\n",
       "0  4  3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
       "1  4  4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
       "2  4  5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
       "3  4  6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
       "4  4  7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
       "\n",
       "                                                   5  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore double quotes with \"quoting = 3\"\n",
    "dataset = pd.read_csv('TextData_One.csv', header=None)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>498.000000</td>\n",
       "      <td>498.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.020080</td>\n",
       "      <td>1867.226908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.699686</td>\n",
       "      <td>2834.891681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>388.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1013.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2366.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>14076.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1\n",
       "count  498.000000    498.000000\n",
       "mean     2.020080   1867.226908\n",
       "std      1.699686   2834.891681\n",
       "min      0.000000      3.000000\n",
       "25%      0.000000    388.250000\n",
       "50%      2.000000   1013.500000\n",
       "75%      4.000000   2366.750000\n",
       "max      4.000000  14076.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 6 columns):\n",
      "0    498 non-null int64\n",
      "1    498 non-null int64\n",
      "2    498 non-null object\n",
      "3    498 non-null object\n",
      "4    498 non-null object\n",
      "5    498 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the texts, just first text \n",
    "create a bag of words; tokenisation process split each review into different relevant words then atribute each word to a column with the amount of times that word shows up  \n",
    "  \n",
    "get only relevant words, i.e. non useful words like;'the', 'on', 'is', '...', '1' etc would be removed  \n",
    "apply steming; taking the root of word e.g. loved -> love, so as not to have too many words  \n",
    "get rid of capitals  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' stellargirl I loooooooovvvvvveee my Kindle   Not that the DX is cool  but the   is fantastic in its own right '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove characters that are not a-z\n",
    "import re\n",
    "review = re.sub('[^a-zA-Z]', ' ', dataset[5][0])\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' stellargirl i loooooooovvvvvveee my kindle   not that the dx is cool  but the   is fantastic in its own right '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make all the letters lowercase\n",
    "review = review.lower()\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stellargirl',\n",
       " 'i',\n",
       " 'loooooooovvvvvveee',\n",
       " 'my',\n",
       " 'kindle',\n",
       " 'not',\n",
       " 'that',\n",
       " 'the',\n",
       " 'dx',\n",
       " 'is',\n",
       " 'cool',\n",
       " 'but',\n",
       " 'the',\n",
       " 'is',\n",
       " 'fantastic',\n",
       " 'in',\n",
       " 'its',\n",
       " 'own',\n",
       " 'right']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert string into list of words\n",
    "review = review.split()\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of irrelevant words like;'the', 'on', 'is', '...', '1' etc\n",
    "import nltk\n",
    "\n",
    "# one could download list of all possible irrelevant words i.e.'stopwords' from 'nltk' \n",
    "#nltk.download('stopwords') \n",
    "# then import 'stopwords' from 'nltk' to use\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply steming; taking the root of a word e.g. loved -> love, so as not to have too many words\n",
    "# so as to aviod too much sparsity\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stellargirl', 'loooooooovvvvvvee', 'kindl', 'dx', 'cool', 'fantast', 'right']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through this list of words to remove irrelevant words while steming each\n",
    "# set function enables the below to run faster\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stellargirl loooooooovvvvvvee kindl dx cool fantast right'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list of words into string seperated by spaces\n",
    "review = ' '.join(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the texts\n",
    "perform text cleaning process on all the text in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus is a collection of text of the same type\n",
    "corpus = []\n",
    "for i in range(0, 498):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset[5][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>ask program latex indesign submit calcio link ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>note hate word hate page hate latex said hate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>ahhh back real text edit environ lt latex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>troubl iran see hmm iran iran far away flockof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>read tweet come iran whole thing terrifi incre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "493  ask program latex indesign submit calcio link ...\n",
       "494  note hate word hate page hate latex said hate ...\n",
       "495          ahhh back real text edit environ lt latex\n",
       "496  troubl iran see hmm iran iran far away flockof...\n",
       "497  read tweet come iran whole thing terrifi incre..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpus).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bag of Words Model\n",
    "take all the unique words to create one column for each word while minimising the amount of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1200) # transforms to sparsed matrix\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  4\n",
       "1  4\n",
       "2  4\n",
       "3  4\n",
       "4  4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ogochukwu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Naive Bayes to the dataset\n",
    "Naive bayes classifies by calculating the probability of being classified in a given category giving the feature (IVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a new result with Naive Bayes\n",
    "All models are wrong but some are useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 0, 4, 4, 0, 4, 2, 0, 0, 0, 2, 4, 2, 4, 4, 4, 0, 0, 4, 4, 4,\n",
       "       4, 0, 2, 4, 2, 0, 2, 2, 4, 4, 0, 4, 4, 2, 4, 4, 4, 4, 2, 2, 0, 2, 2,\n",
       "       0, 4, 2, 0, 2, 0, 0, 0, 4, 4, 4, 2, 2, 4, 4, 4, 0, 4, 0, 4, 4, 0, 0,\n",
       "       0, 0, 2, 0, 2, 0, 0, 2, 4, 4, 4, 2, 4, 4, 2, 0, 2, 2, 4, 4, 0, 4, 0,\n",
       "       2, 2, 4, 0, 4, 4, 4, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 0, 4, 4, 4, 2, 2, 2, 0, 2, 4, 2, 4, 2, 4, 0, 0, 4, 2, 0,\n",
       "       4, 2, 2, 4, 2, 0, 0, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 0, 2, 4,\n",
       "       0, 4, 2, 0, 0, 0, 0, 0, 4, 4, 4, 2, 0, 4, 0, 4, 0, 4, 0, 4, 2, 0, 0,\n",
       "       2, 0, 0, 2, 0, 2, 2, 4, 0, 4, 4, 2, 2, 2, 2, 0, 2, 4, 2, 4, 0, 2, 2,\n",
       "       0, 2, 4, 0, 4, 2, 4, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[138   2   8]\n",
      " [  0 104   0]\n",
      " [  0   3 143]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97       148\n",
      "          2       0.95      1.00      0.98       104\n",
      "          4       0.95      0.98      0.96       146\n",
      "\n",
      "avg / total       0.97      0.97      0.97       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_train, y_pred_train)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[19  6  4]\n",
      " [ 8 17 10]\n",
      " [ 3  5 28]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.66      0.64        29\n",
      "          2       0.61      0.49      0.54        35\n",
      "          4       0.67      0.78      0.72        36\n",
      "\n",
      "avg / total       0.64      0.64      0.63       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, y_pred_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Decision Tree to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a new result with Decision Tree\n",
    "All models are wrong but some are useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 4, 4, 4, 4, 2, 0, 4, 2, 2, 4, 2, 4, 2, 4, 0, 0, 0, 2, 2,\n",
       "       4, 0, 2, 4, 4, 0, 2, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 0, 2, 4, 2, 4,\n",
       "       0, 4, 2, 0, 2, 0, 0, 0, 2, 0, 0, 4, 0, 4, 4, 0, 0, 0, 4, 4, 2, 0, 0,\n",
       "       4, 0, 4, 0, 0, 2, 4, 2, 0, 4, 4, 2, 4, 2, 2, 0, 2, 2, 2, 4, 0, 0, 2,\n",
       "       2, 2, 2, 0, 2, 4, 2, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 0, 4, 4, 4, 2, 2, 2, 0, 2, 4, 2, 4, 2, 4, 0, 0, 4, 2, 0,\n",
       "       4, 2, 2, 4, 2, 0, 0, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 0, 2, 4,\n",
       "       0, 4, 2, 0, 0, 0, 0, 0, 4, 4, 4, 2, 0, 4, 0, 4, 0, 4, 0, 4, 2, 0, 0,\n",
       "       2, 0, 0, 2, 0, 2, 2, 4, 0, 4, 4, 2, 2, 2, 2, 0, 2, 4, 2, 4, 0, 2, 2,\n",
       "       0, 2, 4, 0, 4, 2, 4, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[148   0   0]\n",
      " [  0 104   0]\n",
      " [  0   0 146]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       148\n",
      "          2       1.00      1.00      1.00       104\n",
      "          4       1.00      1.00      1.00       146\n",
      "\n",
      "avg / total       1.00      1.00      1.00       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_train, y_pred_train)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[19  5  5]\n",
      " [ 5 22  8]\n",
      " [ 5  6 25]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.66      0.66        29\n",
      "          2       0.67      0.63      0.65        35\n",
      "          4       0.66      0.69      0.68        36\n",
      "\n",
      "avg / total       0.66      0.66      0.66       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, y_pred_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Random Forest to the dataset\n",
    "All models are wrong but some are useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a new result with Random Forest\n",
    "All models are wrong but some are useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 4, 0, 4, 2, 2, 0, 4, 2, 2, 4, 2, 4, 2, 4, 0, 0, 0, 4, 2,\n",
       "       4, 0, 2, 4, 4, 4, 2, 0, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 0, 2, 4, 2, 4,\n",
       "       0, 4, 2, 0, 4, 0, 0, 0, 4, 4, 4, 4, 0, 4, 4, 4, 0, 4, 4, 0, 2, 0, 0,\n",
       "       2, 0, 4, 0, 0, 2, 4, 0, 2, 4, 4, 2, 4, 2, 2, 0, 2, 2, 2, 2, 0, 4, 2,\n",
       "       2, 2, 2, 0, 2, 4, 4, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 0, 4, 4, 4, 2, 2, 2, 0, 2, 4, 2, 4, 2, 4, 0, 0, 4, 2, 0,\n",
       "       4, 2, 2, 4, 2, 0, 0, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 0, 2, 4,\n",
       "       0, 4, 2, 0, 0, 0, 0, 0, 4, 4, 4, 2, 0, 4, 0, 4, 0, 4, 0, 4, 2, 0, 0,\n",
       "       2, 0, 0, 2, 0, 2, 2, 4, 0, 4, 4, 2, 2, 2, 2, 0, 2, 4, 2, 4, 0, 2, 2,\n",
       "       0, 2, 4, 0, 4, 2, 4, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[147   0   1]\n",
      " [  2 102   0]\n",
      " [  0   0 146]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       148\n",
      "          2       1.00      0.98      0.99       104\n",
      "          4       0.99      1.00      1.00       146\n",
      "\n",
      "avg / total       0.99      0.99      0.99       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_train, y_pred_train)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[17  5  7]\n",
      " [ 5 21  9]\n",
      " [ 4  5 27]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.59      0.62        29\n",
      "          2       0.68      0.60      0.64        35\n",
      "          4       0.63      0.75      0.68        36\n",
      "\n",
      "avg / total       0.65      0.65      0.65       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, y_pred_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting K-NN to the dataset\n",
    "All models are wrong but some are useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a new result with K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 0, 4, 0, 2, 0, 0, 0, 4, 2, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0,\n",
       "       0, 2, 0, 4, 4, 0, 0, 0, 2, 4, 4, 2, 4, 2, 0, 0, 4, 2, 2, 2, 0, 4, 4,\n",
       "       0, 4, 2, 0, 4, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 4, 0, 0,\n",
       "       0, 2, 2, 0, 2, 0, 2, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 0, 4, 4, 4, 2, 2, 2, 0, 2, 4, 2, 4, 2, 4, 0, 0, 4, 2, 0,\n",
       "       4, 2, 2, 4, 2, 0, 0, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 0, 2, 4,\n",
       "       0, 4, 2, 0, 0, 0, 0, 0, 4, 4, 4, 2, 0, 4, 0, 4, 0, 4, 0, 4, 2, 0, 0,\n",
       "       2, 0, 0, 2, 0, 2, 2, 4, 0, 4, 4, 2, 2, 2, 2, 0, 2, 4, 2, 4, 0, 2, 2,\n",
       "       0, 2, 4, 0, 4, 2, 4, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "checking for over fitting by comparing trained and tested matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[128  12   8]\n",
      " [ 24  69  11]\n",
      " [ 49  18  79]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.86      0.73       148\n",
      "          2       0.70      0.66      0.68       104\n",
      "          4       0.81      0.54      0.65       146\n",
      "\n",
      "avg / total       0.71      0.69      0.69       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_train, y_pred_train)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[25  1  3]\n",
      " [19 14  2]\n",
      " [12 12 12]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.86      0.59        29\n",
      "          2       0.52      0.40      0.45        35\n",
      "          4       0.71      0.33      0.45        36\n",
      "\n",
      "avg / total       0.57      0.51      0.49       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, y_pred_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Support Vector Machine (SVM) to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear', random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a new result with Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 4, 4, 0, 2, 0, 0, 4, 4, 2, 0, 2, 4, 4, 4, 0, 0, 0, 4, 2,\n",
       "       0, 0, 2, 4, 4, 0, 2, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 2, 2, 2, 2, 4,\n",
       "       0, 4, 2, 0, 2, 0, 0, 0, 4, 4, 4, 4, 0, 4, 4, 0, 0, 4, 0, 0, 2, 0, 0,\n",
       "       2, 0, 4, 0, 0, 2, 4, 0, 0, 4, 4, 2, 0, 2, 2, 0, 2, 2, 4, 2, 4, 4, 2,\n",
       "       2, 2, 4, 0, 4, 4, 2, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 0, 4, 4, 4, 2, 2, 2, 0, 2, 4, 2, 4, 2, 4, 0, 0, 4, 2, 0,\n",
       "       4, 2, 2, 4, 2, 0, 0, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 0, 2, 4,\n",
       "       0, 4, 2, 0, 0, 0, 0, 0, 4, 4, 4, 2, 0, 4, 0, 4, 0, 4, 0, 4, 2, 0, 0,\n",
       "       2, 0, 0, 2, 0, 2, 2, 4, 0, 4, 4, 2, 2, 2, 2, 0, 2, 4, 2, 4, 0, 2, 2,\n",
       "       0, 2, 4, 0, 4, 2, 4, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "checking for over fitting by comparing trained and tested matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[147   0   1]\n",
      " [  0 104   0]\n",
      " [  0   1 145]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       148\n",
      "          2       0.99      1.00      1.00       104\n",
      "          4       0.99      0.99      0.99       146\n",
      "\n",
      "avg / total       0.99      0.99      0.99       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_train, y_pred_train)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[19  5  5]\n",
      " [ 5 20 10]\n",
      " [ 7  4 25]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.66      0.63        29\n",
      "          2       0.69      0.57      0.62        35\n",
      "          4       0.62      0.69      0.66        36\n",
      "\n",
      "avg / total       0.64      0.64      0.64       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, y_pred_test)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
